---
title: "R4DS_code_Program(17-21)"
author: "Pete VZ"
date: "`r Sys.Date()`"
output: html_document
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This file contains code & workthroughs of the 1sd Ed of the R For Data Science book (R4DS): https://r4ds.had.co.nz/index.html. Each chapter will be indexed with a heading. There will also be notes on the book's information so that the exported Rmd file or the code below can be searched for particular topics (e.g., tidy data). 

Also, consult the solutions manual: https://jrnold.github.io/r4ds-exercise-solutions/index.html


This file is on chapters 17-21 and it covers pipes, functions, vectors, and iteration. Ch 17 is just introduction.

```{r}
#install packages
library(tidyverse)
```


# ----*Chapter 18: Pipes*-----

The pipe is extremely useful, and you can find more of a discussion about the magrittr pipe vs the newly native pipe here: https://r4ds.hadley.nz/workflow-pipes.html 
To see why the pipe is so useful, Hadley is going to tell us a story about little rabbit Foo Foo.


We’ll start by defining an object to represent little bunny Foo Foo. And we’ll use a function for each key verb: hop(), scoop(), and bop(). Using this object and these verbs, there are (at least) four ways we could retell the story in code:

1. Save each intermediate step as a new object.
2. Overwrite the original object many times.
3. Compose functions.
4. Use the pipe.
We’ll work through each approach, showing you the code and talking about the advantages and disadvantages.
```{r}
#don't bother trying to run this now
foo_foo <- little_bunny()
```

#### 18.2.1 Intermediate steps as a solution

```{r}
foo_foo_1 <- hop(foo_foo, through = forest)
foo_foo_2 <- scoop(foo_foo_1, up = field_mice)
foo_foo_3 <- bop(foo_foo_2, on = head)
```

This requires you to come up with intermediate names for each step, and that's not terrible if they can make sense, but if not (as in this example), you develop names that are hard to remember and become cluttered.

You may worry that R will run out of room if you do too much of this variable naming thing, but the Had-man suggest you worry about that way down the road, since R is plenty smart at minimizing memory use. As an example:
```{r}
diamonds <- ggplot2::diamonds
diamonds2 <- diamonds %>% 
  dplyr::mutate(price_per_carat = price / carat)

pryr::object_size(diamonds)
#> Registered S3 method overwritten by 'pryr':
#>   method      from
#>   print.bytes Rcpp
#> 3.46 MB
pryr::object_size(diamonds2)
#> 3.89 MB
pryr::object_size(diamonds, diamonds2)
#> 3.89 MB
```

Why are both datasets together not 2x the size? This is because R only duplicates columns that aren't shared among dataframes. Bottom line: you need to do a lot of modifying or have a very large dataset for this to be a problem. The biggest issues with option 1 is that it's clunky, primes you for mistakes, and is hard to read/write.

##### 18.2.2 overwriting the original df as a solution

```{r}
foo_foo <- hop(foo_foo, through = forest)
foo_foo <- scoop(foo_foo, up = field_mice)
foo_foo <- bop(foo_foo, on = head)
```

This is less typing, but it can wrap you up in different types of mistakes:
1. Debugging is painful: if you make a mistake you’ll need to re-run the complete pipeline from the beginning.

2. The repetition of the object being transformed (we’ve written foo_foo six times!) obscures what’s changing on each line.

##### 18.2.3 stringing function calls together as a solution

```{r}
bop(
  scoop(
    hop(foo_foo, through = forest),
    up = field_mice
  ), 
  on = head
)
```

Here the disadvantage is that you have to read from inside-out, from right-to-left, and that the arguments end up spread far apart. In short, this code is hard for a human to consume.

##### 18.2.4 The best solution: pipes

```{r}
foo_foo %>%
  hop(through = forest) %>%
  scoop(up = field_mice) %>%
  bop(on = head)
```

The advantages are abundant here: It focuses on verbs (what is being done to the data), it's easy to read start from finish (like a pipeline), and it's easy to conceptualize.

Under the hood, magrittr reassembles the code in the pipe to a form that works by overwriting an intermediate object. When you run a pipe like the one above, magrittr does something like this:
```{r}
#note that the . is a temporary placeholder
my_pipe <- function(.) {
  . <- hop(., through = forest)
  . <- scoop(., up = field_mice)
  bop(., on = head)
}
my_pipe(foo_foo)
```

There are 2 classes of functions that can't be piped:
1. Functions that use the current environment. For example, assign() will create a new variable with the given name in the current environment. Same goes for get() & load().

Here, the use of assign with the pipe does not work because it assigns it to a temporary environment used by %>%. If you do want to use assign with the pipe, you must be explicit about the environment:
```{r}
env <- environment()
"x" %>% assign(100, envir = env)
x
#> [1] 100
```

2. Functions that use lazy evaluation. In R, function arguments are only computed when the function uses them, not prior to calling the function. The pipe computes each element in turn, so you can’t rely on this behavior.

#### 18.3 When to avoid the pipe

Pipes work best when the number of operations to perform are fairly small and what you want to do isn't too complicated. Here's when another option might be better:
1. If you're working on > 10 steps, go with the option where you're using intermediate names. This is better b.c. you can debug/check intermediate steps and you can choose names that help the reader understand your intentions.
2. You have > 1 inputs or outputs. Pipes work best when you've got one input (one df) and one output you're shooting for (a summary table of grouped variables). A grouping plus a figure counts as one output for these purposes.
3. If your output is sufficiently complex, go another route.


# ----*Chapter 19: Functions in R*-----

The job of functions is to automate tasks in a powerful and general way that can be applied repeatedly to a variety of situations. A major strength of functions is that as requirements change, you only need to change the function and rerun it, as opposed to rewriting and re-running the code chunks over and over. Moreover, the minimize the chance of silly mistakes, like missing a character or introducing a typo.

This chapter pays particular to coding style, which improves readability, functionality, and usability.


#### 19.2 When to use a function

The most fundamental rule is to write a function whenever you have a block of code that needs repeating > 2 times. For example, what does this code do? (it scales each column in some way)
```{r}
df <- tibble::tibble(
  a = rnorm(10),
  b = rnorm(10),
  c = rnorm(10),
  d = rnorm(10)
)

df$a <- (df$a - min(df$a, na.rm = TRUE)) / 
  (max(df$a, na.rm = TRUE) - min(df$a, na.rm = TRUE))
df$b <- (df$b - min(df$b, na.rm = TRUE)) / 
  (max(df$b, na.rm = TRUE) - min(df$a, na.rm = TRUE)) #note the typo here
df$c <- (df$c - min(df$c, na.rm = TRUE)) / 
  (max(df$c, na.rm = TRUE) - min(df$c, na.rm = TRUE))
df$d <- (df$d - min(df$d, na.rm = TRUE)) / 
  (max(df$d, na.rm = TRUE) - min(df$d, na.rm = TRUE))
```

Writing a function minimizes the chance of these errors, and reduces time as the amount of your data grows. 

To rewrite this as a function, the first thing to do is analyze what it intends to do. Here, how many inputs does it have?
```{r}
(df$a - min(df$a, na.rm = TRUE)) /
  (max(df$a, na.rm = TRUE) - min(df$a, na.rm = TRUE))
```

Even though it is initially pretty ugly looking and complicated, it looks like it only requires 1 input: df$a.

##### 1.rewrite with variables
To break down the code chunk and transition toward a function, start by making the inputs more clear. Step 1 is to rewrite the code using temporary variables with general names. Here this code only requires a single numeric vector, so we’ll call it x:
```{r}
x <- df$a
(x - min(x, na.rm = TRUE)) / 
  (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))
```

##### 2. remove duplication
Is there any calculation that is repeated in the code? If so, make it a temporary variable to clarify what specifically the code is doing.

Here, we’re computing the range of the data three times, so it makes sense to do it in one step using the range() function, which returns a 2 element vector containing the minimum and maximum of all the given arguments. You can see this in the example below.
```{r}
range(df$a)
```

The first element ouput by range is the minimum, and the second element is the maximum, so if we assign rng to the range(df$a), then calling rng[1] gives us the min & rng[2] gives the max. Rewriting the code gives:
```{r}
rng <- range(x, na.rm = TRUE)
(x - rng[1] / (rng[2] - rng[1]))
```

##### 3. turn it into a function (w/ key parts)

There are 3 key parts of a function:
1. Pick a name for the function that is meaningful, obvious w/o too much explanation, and accurately represents what the function does.
2. You list the inputs, or arguments, to the function inside function().
3. Place the code you have developed in body of the function, a { block that immediately follows function(...).
```{r}
rescale01 <- function(x) {
  rng <- range(x, na.rm = TRUE)
  (x - rng[1]) / (rng[2] - rng[1])
}
rescale01(c(0, 5, 10))
#> [1] 0.0 0.5 1.0
```

In the example above, 
1. the function is called "rescale01" b.c. it rescales a vector to between 0 and 1.
2. the function only requires one argument (x). If we needed more, the call would look like (x, y, z)
3. we use the code built up in previous steps to plug into the braces {}. 

This illustrates the best and easiest way to build a function: figure out how to make it work for simple input, then turn it into a function. The backwards way is to create the function first then figure out how to make it work (I need more examples to better internalize this).

##### checking the function w/ other inputs

To test it, throw a range of inputs at the function.
```{r}
rescale01(c(3, 5, 7, 9, -1))
rescale01(c(1, 2, 3, NA, 5))
```

##### moving towards packages
As you write more and more functions you’ll eventually want to convert these informal, interactive tests into formal, automated tests. That process is called unit testing. Unfortunately, it’s beyond the scope of this book, but you can learn about it in http://r-pkgs.had.co.nz/tests.html, which is a chapter in the awesome looking book on R packages: https://r-pkgs.org/index.html.

Returning to the initial problem, we can use the function to rescale the 4 arrays:
```{r}
df$a <- rescale01(df$a)
df$b <- rescale01(df$b)
df$c <- rescale01(df$c)
df$d <- rescale01(df$d)
```

This is still a bit redundant, but Had will address this after we've learned more about R's vectors.

Another advantage of functions is that if our requirements change, we only need to make the change in one place. For example, we might discover that some of our variables include infinite values, and rescale01() fails:
```{r}
x <- c(1:10, Inf)
rescale01(x)
```

Because we’ve extracted the code into a function, we only need to make the fix in one place:
```{r}
rescale01 <- function(x) {
  rng <- range(x, na.rm = TRUE, finite = TRUE) #update to address this unexpected case
  (x - rng[1]) / (rng[2] - rng[1])
}
rescale01(x)
```

##### 19.2.1 Exercises on functions

2. In the second variant of rescale01(), infinite values are left unchanged. Rewrite rescale01() so that -Inf is mapped to 0, and Inf is mapped to 1.
```{r 19.2 rescale funct}
rescale01 <- function(x) {
  rng <- range(x, na.rm = TRUE, finite = TRUE) 
  y <- (x - rng[1]) / (rng[2] - rng[1]) #assign the output to y so it can be modified
  y[y == -Inf] <- 0 #I don't follow this syntax, but I think it's saying
  #that where y is -Inf, assign it to 0, and
  y[y == Inf] <- 1 #where y is Inf, assign it to 1. Why not a ifelse?
  y
}
rescale01(x)
```

3. Practice turning the following code snippets into functions. Think about what each function does. What would you call it? How many arguments does it need? Can you rewrite it to be more expressive or less duplicative?
 a. mean(is.na(x))
 b. x / sum(x, na.rm = TRUE)
 c. sd(x, na.rm = TRUE) / mean(x, na.rm = TRUE)
 
 3a. This looks like it tells you the proportion of values in a vector that are missing (NA). It already has a variable, so x will be our single input argument. There isn't any duplication, so let's try to make the function:
 
First, playing around with what it does:
```{r}
x <- c(2, 4, 0, NA, 4)
mean(is.na(x))
```

```{r}
prop_NAs <- function(x) {
  mean(is.na(x))
}
prop_NAs(x)
```

3b. x / sum(x, na.rm = TRUE) 
This takes a vector, adds it up, and calculates the proportion of each element as a part of the vector (ignoring NAs)
```{r}
fract_array <- function(x) {
  x / sum(x, na.rm = TRUE)
}
fract_array(x)
sum(x, na.rm = TRUE)
fract_array(c(1:5, NA))
```

3c. sd(x, na.rm = TRUE) / mean(x, na.rm = TRUE)
This divides the sd by the mean, which is the CV. The input argument is still just x. There isn't really any redundancy that can be eliminated.
```{r}
calc_cv <- function(x) {
  sd(x, na.rm = TRUE) / mean(x, na.rm = TRUE)
}
calc_cv(x)
```

4. Write a function to calculate the variance and skewness of a numeric vector. Book gives the equations. This one is pretty hard (too hard?) It can be useful to see the walk-through here: https://nicercode.github.io/intro/writing-functions.html

For the variance equation, we need a couple of pieces: the number of obs in the vector (1/n-1) and the average (xbar). 

```{r}
#this is the vector we'll work with for now
x <- c(2, 4, 40, 1, 3)
sum(x)
```

```{r}
#the length is simple. We'll need it for later, so assign it to a variable
n <- length(x)
# given that, the first part of the calculation will be
n
1/(n - 1)
```

The average (xbar) is also easy, since that's just mean(x). We'll assign that to a variable as well so it can be used later.
```{r}
#for now, x doesn't have NAs in it, but we'll need that in testing
#also, introduce negative numbers (seems to work as is)
xbar <- mean(x)
x - xbar
```

Each of those differences need to be squared.
```{r}
(x - xbar)^2
```

Then sum them all up
```{r}
sum((x - xbar)^2)
```

Put the variance function together:
```{r 4-variance funct}
var_array <- function(x) {
  n <- length(x) #save the length for later use
  xbar <- mean(x) #save the mean for later use
  (1/(n - 1)) *  sum((x - xbar)^2)
}
var_array(x)
var(x) #check to see that my output matches R's function (and both fail when NA's exist)
```

Now for the skewness:
We need several pieces, some of which we've already defined. We need 1/(n-2), sum of all xi - xbar cubed, and the variance to the 3/2 power.
```{r skewness function}
#it needs to be independent of the variance function, so redefine all vars w/in this function
skew <- function(x) {
  n <- length(x) #save the length for later use
  xbar <- mean(x) #save the mean for later use
  var_x <- var(x) #save variance for later
  #(1/(n - 2)) = a piece
  #sum((x - xbar)^3) = a second piece
  ((1/(n - 2)) * sum((x - xbar)^3)) / (var_x) ^ (3/2)
}
skew(x)
skew(c(1, 2, 5, 100)) #this answer matches the sol'n page
```

5. Write both_na(), a function that takes two vectors of the same length and returns the number of positions that have an NA in both vectors.
What we need: 1) check if len vector a = length of vector b, 2) a count of NA in a, count of NA in b

```{r 19.2.5 both_na function}
#checks if they're same length
#ifelse(length(a) == length(b), do this, else do this)

#part 2 (do this if true)
#sum(is.na(a))
#sum(is.na(b))

#put them together
both_na <- function(a, b) {
  na_a <- sum(is.na(a))
  na_b <- sum(is.na(b))
  ifelse(length(a) == length(b), sum(na_a, na_b))
}

a <- c(1, 3, 5, 7)
b <- c(2, 4, 6, NA)

both_na(a, b)
#output <- c((length(a)), (length(b)))
#output
```

My solution addresses the problem of how many total NAs are found in total in both vectors. The sol'n page took a different interpretation
```{r 19.2.5 sol'n}
#this answers how many entries in both vectors have NAs in the same position?
both_na_soln <- function(x, y) {
  sum(is.na(x) & is.na(y)) # I don't see how this does the check
  #ahh, I see: for each instance x (x1, x2, etc.), is.na evaluates the cell to true (1) or
  # false (0). The & then asks whether x1 and y1 are both true at the same time.
  # If x1 & y1 are both true, then TRUE = 1. Add up all TRUEs with sum()
}

both_na_soln(
  c(NA, NA, 1, 2),
  c(NA, 1, NA, 2)
)
#> this should be 1 (the first row)
both_na_soln(
  c(NA, NA, 1, 2, NA, NA, 1),
  c(NA, 1, NA, 2, NA, NA, 1)
)
# should be 3 (rows 1, 5, & 6)

#some testing
is.na(c(NA, NA, 1, 2, NA, NA, 1))
sum(is.na(c(NA, NA, 1, 2, NA, NA, 1)))
```

6. What do the following functions do? Why are they useful even though they are so short?
```{r}
is_directory <- function(x) file.info(x)$isdir
is_readable <- function(x) file.access(x, 4) == 0

#is_directory checks to see if a particular file is in a directory
#is_readable tests whether a file has permissions that allow editing
#both function names are clearer than the base functions they employ
```

7. skipping this one

#### 19.3 coding style (functions are for people & computers)

* The name of a function is important. Ideally, the name of your function will be short, but clearly evoke what the function does. That’s hard! But it’s better to be clear than short, as RStudio’s autocomplete makes it easy to type long names.

* Generally, function names should be verbs, and arguments should be nouns (objects). For example, in pivot_longer(x), the name says what it does, and x tells you what it works on.

* The object of the function can be included in the name, as clarity is more important than length. For example, impute_missing() or collapse_years().

* if they need to be long, use snake_case and avoid CamelCase or period.case. R isn't very consistent, but that doesn't mean you shouldn't be.

* if you have a family of functions that do similar things (like pivot_wider() & pivot_longer), give them consistent names and arguments. Also make sure to use consistent prefixes since autocomplete allows you to see functions of a similar family. Another good example is the stringr family, where all functions start with str_.

* try to avoid overwriting existing functions if possible, especially base R functions. 

* Use comments to explain "why" you're doing something and try to avoid the "how" or "what" (though I'm going to err on the side of more comments for now). It's also good to break up the function into component parts, e.g.,:
```{r}
# Load data --------------

# Plot data --------------
```

##### 19.3.1 Exercises
1a. This seems to check for matches between a string and a prefix. It returns true or false. A better name would be something like check_prefix()
f1 <- function(string, prefix) {
  substr(string, 1, nchar(prefix)) == prefix
}

1b. This drops the last element of a vector. Better name: drop_last()
f2 <- function(x) {
  if (length(x) <= 1) return(NULL)
  x[-length(x)]
}

1c. This repeats y for each element of x. Better is: recycle()
f3 <- function(x, y) {
  rep(y, length.out = length(x))
}


##### 19.4 Conditional execution (if)

An if statement allows you to conditionally execute code.
```{r}
if (condition) {
  # code executed when condition is TRUE
} else {
  # code executed when condition is FALSE
}
```

Here’s a simple function that uses an if statement. The goal of this function is to return a logical vector describing whether or not each element of a vector is named.
```{r}
has_name <- function(x) {
  nms <- names(x)
  if (is.null(nms)) {
    rep(FALSE, length(x))
  } else {
    !is.na(nms) & nms != ""
  }
}

has_name(diamonds)
```


##### 19.4.1 Conditions

When using conditionals, the condition must evaluate to either TRUE or FALSE. (There's a lot more here, but they seem like a bunch of trivial exceptions that I don't understand the context of.)


##### 19.4.2 Multiple conditions
You can chain multiple if statements together:
```{r}
if (this) {
  # do that
} else if (that) {
  # do something else
} else {
  # 
}
```

If you have too many if...else statements together it can get messy. One trick is to use the switch() function. See exercises below for info. 


##### 19.4.3 Code style
Both if and function should (almost) always be followed by squiggly brackets ({}), and the contents should be indented by two spaces. This makes it easier to see the hierarchy in your code by skimming the left-hand margin.

An opening curly brace should never go on its own line and should always be followed by a new line. A closing curly brace should always go on its own line, unless it’s followed by else. Always indent the code inside curly braces.
```{r}
# Good
if (y < 0 && debug) {
  message("Y is negative")
}

if (y == 0) {
  log(x)
} else {
  y ^ x
}

# Bad
if (y < 0 && debug)
message("Y is negative")

if (y == 0) {
  log(x)
} 
else {
  y ^ x
}
```

##### 19.4.4 Exercises
1. Difference between ifelse() and if?
If tests a single condition, but ifelse() tests each element of a vector

2. Write a greeting function that says “good morning”, “good afternoon”, or “good evening”, depending on the time of day. (Hint: use a time argument that defaults to lubridate::now(). That will make it easier to test your function.)
```{r}
#stolen from sol'n
library(lubridate)
greet <- function(time = lubridate::now()) {
  hr <- lubridate::hour(time)
  # I don't know what to do about times after midnight,
  # are they evening or morning?
  if (hr < 12) {
    print("good morning")
  } else if (hr < 17) {
    print("good afternoon")
  } else {
    print("good evening")
  }
}
greet()
#> [1] "good morning"
greet(ymd_h("2017-01-08:05"))
#> [1] "good morning"
greet(ymd_h("2017-01-08:13"))
#> [1] "good afternoon"
greet(ymd_h("2017-01-08:20"))
#> [1] "good evening"
```

3. Implement a fizzbuzz() function. It takes a single number as input. If the number is divisible by three, it returns “fizz”. If it’s divisible by five it returns “buzz”. If it’s divisible by three and five, it returns “fizzbuzz”. Otherwise, it returns the number. Make sure you first write working code before you create the function.

Taking this in pieces. At some point, we'll need an if...else structure. For now, just do one piece at a time.
```{r fizzbuz function}
#step 1 - divisible by 3 (w/ no remainder) say "fizz"
# if (x %% 3 == 0)

#step 2 - same for divisible by 5 say "buzz"
# if (x %% 5 == 0)

#step 3 - divisible by both, say "fizzbuzz"
# if (x %% 3 == 0 && x %% 5 == 0)

#step 4 - put into if...else
x <- 15
if (x %% 3 == 0 && x %% 5 == 0) {
  print("fizzbuzz")
} else if (x %% 3 == 0) {
  print("fizz")
} else if (x %% 5 == 0) {
  print("buzz")
} else {
  x
}

#finally, put it into a function
fizzbuzz <- function(x) {
  if (x %% 3 == 0 && x %% 5 == 0) {
  print("fizzbuzz")
} else if (x %% 3 == 0) {
  print("fizz")
} else if (x %% 5 == 0) {
  print("buzz")
} else {
  x
}
}

fizzbuzz(6961)
```

##### trick for checking divisibility
A more concise way of checking for divisibility is to note that the not operator will return TRUE for 0, and FALSE for all non-zero numbers. Thus, !(x %% y), will check whether y divides x.
```{r}
#these produce the same output
(1:10 %% 3 == 0)
!(1:10 %% 3)
```

Apparently, this problem is both simple and lends itself to many solutions (at least 10, as found in this book: https://joelgrus.com/2020/06/06/ten-essays-on-fizz-buzz/, and in the solution page: https://jrnold.github.io/r4ds-exercise-solutions/functions.html#exercise-19.4.3). For these reasons, it is a common interview question that evaluates a programmer's creativity and quality.


4. How could you use cut() to simplify this set of nested if-else statements?
```{r 19.4.4 using cut()}
#here's the example:
if (temp <= 0) {
  "freezing"
} else if (temp <= 10) {
  "cold"
} else if (temp <= 20) {
  "cool"
} else if (temp <= 30) {
  "warm"
} else {
  "hot"
}
```

Attempt at using cut():
```{r}
# the solution page has an answer & explanation, but I don't get either
# however, cut() works on vectors, so there's probably some value there
```

5. What happens if you use switch() with numeric values?
According to sol'n, In switch(n, ...), if n is numeric, it will return the nth argument from .... This means that if n = 1, switch() will return the first argument in ..., if n = 2, the second, and so on. For example,
```{r 19.4.5 using switch()}
switch(1, "apple", "banana", "cantaloupe")
#> [1] "apple"
switch(2, "apple", "banana", "cantaloupe")
#> [1] "banana"
```

6. What does this switch() call do? What happens if x is "e"?

According to the documentation, the usage is switch(EXPR, ...), where switch evaluates EXPR and accordingly chooses one of the further arguments (in ...). 

Furthermore, if EXPR evaluates to a character string then that string is matched (exactly) to the names of the elements in .... If there is a match then that element is evaluated unless it is missing, in which case the next non-missing element is evaluated
```{r 19.4.5 more switch()}
x <- "b"
switch(x,
  a = ,
  b = "ab",
  c = ,
  d = "cd"
)
x
```


#### 19.5 Function arguments

The arguments to a function typically fall into two broad sets: one set supplies the data to compute on, and the other supplies arguments that control the details of the computation. Generally, data arguments should come first. Detail arguments should go on the end, and usually should have default values. You specify a default value in the same way you call a function with a named argument:
```{r}
# Compute confidence interval around mean using normal approximation
mean_ci <- function(x, conf = 0.95) {
  se <- sd(x) / sqrt(length(x))
  alpha <- 1 - conf
  mean(x) + se * qnorm(c(alpha / 2, 1 - alpha / 2))
}

x <- runif(100)
mean_ci(x)
#> [1] 0.4976111 0.6099594
mean_ci(x, conf = 0.99)
#> [1] 0.4799599 0.6276105

mean_ci(x, conf = .5)
```

##### 19.5.1 abbreviations for function arguments

Memorize & use these common variables for arguments, since they're used by convention:
* x, y, z: vectors.
* w: a vector of weights.
* df: a data frame.
* i, j: numeric indices (typically rows and columns).
* n: length, or number of rows.
* p: number of columns.


##### 19.5.2 Checking values

As you make and reuse more personal functions, it becomes more important to put checks in place so they work correctly with a range of arguments or inputs. To avoid this problem, it’s often useful to make constraints explicit. For example, imagine you’ve written some functions for computing weighted summary statistics:
```{r}
wt_mean <- function(x, w) {
  sum(x * w) / sum(w)
}
wt_var <- function(x, w) {
  mu <- wt_mean(x, w)
  sum(w * (x - mu) ^ 2) / sum(w)
}
wt_sd <- function(x, w) {
  sqrt(wt_var(x, w))
}
```

What happens if x and w are not the same length? (they should be for these calculations)
```{r}
wt_mean(1:6, 1:3)
```

In this case, because of R’s vector recycling rules, we don’t get an error.

It’s good practice to check important preconditions, and throw an error (with stop()), if they are not true:
```{r}
#rewriting the function to limit bad inputs
wt_mean <- function(x, w) {
  if (length(x) != length(w)) {
    stop("`x` and `w` must be the same length", call. = FALSE)
  }
  sum(w * x) / sum(w)
}
```

```{r}
wt_mean(1:6, 1:3)
```

You can get carried away on this, thinking of a lot of contingencies and building a lot of specific feedback text. This can be way too mucy extra work for little additional gain. A useful compromise is the built-in stopifnot(): it checks that each argument is TRUE, and produces a generic error message if not.
```{r}
wt_mean <- function(x, w, na.rm = FALSE) {
  stopifnot(is.logical(na.rm), length(na.rm) == 1)
  stopifnot(length(x) == length(w))
  
  if (na.rm) {
    miss <- is.na(x) | is.na(w)
    x <- x[!miss]
    w <- w[!miss]
  }
  sum(w * x) / sum(w)
}

vec1 <- c(1, 3, 5, 9)
vec2 <- c(2, 4, 6, 8)
wt_mean(vec1, vec2, na.rm = "foo")

# I do NOT get this and I don't understand why you'd write this chunk this way
```

##### 19.5.3 Dot-dot-dot (…)

Many functions in R take an arbitrary number of inputs, for example sum(), c(), etc.
```{r}
sum(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)

stringr::str_c("a", "b", "c", "d", "e", "f")
```

How do these functions work? They rely on a special argument: ... (pronounced dot-dot-dot). This special argument captures any number of arguments that aren’t otherwise matched.

It’s useful because you can then send those ... on to another function. This is a useful catch-all if your function primarily wraps another function. For example, Had commonly creates these helper functions that wrap around str_c():
```{r}
commas <- function(...) stringr::str_c(..., collapse = ", ")
commas(letters[1:10])
#> [1] "a, b, c, d, e, f, g, h, i, j"

rule <- function(..., pad = "-") {
  title <- paste0(...)
  width <- getOption("width") - nchar(title) - 5
  cat(title, " ", stringr::str_dup(pad, width), "\n", sep = "")
}
rule("Important output")
#> Important output -----------------------------------------------------------
```

Here ... lets me forward on any arguments that I don’t want to deal with to str_c(). It’s a very convenient technique. But it does come at a price: any misspelled arguments will not raise an error. This makes it easy for typos to go unnoticed:
```{r}
x <- c(1, 2, 3)
sum(x, na.mr = TRUE) #note that this should be na.rm = TRUE & I'm not sure why it's being interpreted as a value = 1
#> [1] 4
```


##### 19.5.4 Lazy evaluation

Arguments in R are lazily evaluated: they’re not computed until they’re needed. That means if they’re never used, they’re never called. This is an important property of R as a programming language, but is generally not important when you’re writing your own functions for data analysis. You can read more about lazy evaluation at http://adv-r.had.co.nz/Functions.html#lazy-evaluation.

##### 19.5.5 Exercises

1. What does commas(letters, collapse = "-") do? Why?
```{r}
commas(letters, collapse = "-")
```
There is a problem with ... here in str_c() for some reason. Sol'n says:
The commas() function in the chapter is defined as

commas <- function(...) {
  str_c(..., collapse = ", ")
}

This is because when the argument collapse is given to commas(), it is passed to str_c() as part of .... In other words, the previous code is equivalent to

str_c(letters, collapse = "-", collapse = ", ")

So the issue is that it is an error to give the same named argument to a function twice.

2. It’d be nice if you could supply multiple characters to the pad argument, e.g. rule("Title", pad = "-+"). Why doesn’t this currently work? How could you fix it?

```{r}
rule <- function(..., pad = "+") {
  title <- paste0(...)
  width <- getOption("width") - nchar(title) - 5
  cat(title, " ", stringr::str_dup(pad, width), "\n", sep = "")
}
rule("Important output")
```

This works, but it isn't the desired width b.c. there are 2 characters, making the rule twice as long. To control this, you need to provide some feedback so the function produces the correct length.
```{r}
rule <- function(..., pad = "-") {
  title <- paste0(...)
  width <- getOption("width") - nchar(title) - 5
  padding <- str_dup( #here's where the feedback comes in
    pad,
    ceiling(width / str_length(title))
  ) %>%
    str_trunc(width)
  cat(title, " ", padding, "\n", sep = "")
}
rule("Important output")
#> Important output ----
rule("Valuable output", pad = "-+")
#> Valuable output -+-+-+-+
rule("Vital output", pad = "-+-")
#> Vital output -+--+--+--+--+--+-
```

3. What does the trim argument to mean() do? When might you use it?

```{r}
mean()
```

According to the help, trim controls the fraction (0 to 0.5) of observations to be trimmed from each end of x (sorted) before the mean is computed. Values of trim outside that range are taken as the nearest endpoint. This makes the mean less sensitive to outliers. Interestingly, the median is an extreme trimmed mean, in which all observations are removed except one or two. See this page for more info: https://garstats.wordpress.com/2017/11/28/trimmed-means/.

One conclusion from ^^ is, "Thus, for asymmetric distributions, trimmed means provide more accurate information about central tendency than the mean." Trimmed means are also known as "winsorized" means.

4. The default value for the method argument to cor() is c("pearson", "kendall", "spearman"). What does that mean? What value is used by default?
These names are methods of calculating the correlation coefficient. The default is pearson. Note that you can also control what values are used or avoided (e.g., NAs) with the use = argument.
```{r}
cor()
```


#### 19.6 Return values

Figuring out what your function should return is usually straightforward: it’s why you created the function in the first place! There are two things you should consider when returning a value:

1. Does returning early make your function easier to read?
2. Can you make your function pipeable?

##### 19.6.1 Explicit return statements
The value returned by the function is usually the last statement it evaluates, but you can choose to return early by using return(). I think it’s best to save the use of return() to signal that you can return early with a simpler solution. A common reason to do this is because the inputs are empty:
Here's a mock-up example:
```{r}
complicated_function <- function(x, y, z) {
  if (length(x) == 0 || length(y) == 0) {
    return(0)
  }
    
  # Complicated code here
}
```

Another reason is because you have a if statement with one complex block and one simple block. For example, you might write an if statement like this:

```{r}
f <- function() {
  if (x) {
    # Do 
    # something
    # that
    # takes
    # many
    # lines
    # to
    # express
  } else {
    # return something short
  }
}
```

But if the first block is very long, by the time you get to the else, you’ve forgotten the condition. One way to rewrite it is to use an early return for the simple case and put that first:
```{r}
f <- function() {
  if (!x) {
    return(something_short)
  }

  # Do 
  # something
  # that
  # takes
  # many
  # lines
  # to
  # express
}
```


##### 19.6.2 Writing pipeable functions

If you want to write your own pipeable functions, it’s important to think about the return value. Knowing the return value’s object type will mean that your pipeline will “just work”. For example, with dplyr and tidyr the object type is the data frame.

There are two basic types of pipeable functions: transformations and side-effects. With transformations, an object is passed to the function’s first argument and a modified object is returned. With side-effects, the passed object is not transformed. Instead, the function performs an action on the object, like drawing a plot or saving a file. Side-effects functions should “invisibly” return the first argument, so that while they’re not printed they can still be used in a pipeline. For example, this simple function prints the number of missing values in a data frame:
```{r}
show_missings <- function(df) {
  n <- sum(is.na(df))
  cat("Missing values: ", n, "\n", sep = "") #this concatenates & prints
  
  invisible(df) #this returns an invisible copy of an object
}
```


```{r}
show_missings(mtcars)
```
Here, mtcars was returned, but didn't get printed out. But it’s still there, it’s just not printed by default:
```{r}
x <- show_missings(mtcars) #this captures the hidden output as a variable
#> Missing values: 0
class(x)
#> [1] "data.frame"
dim(x)
#> [1] 32 11
```

AND, it can be used in a pipe (runs through mtcars twice to generate NAs to count):
```{r}
y <- mtcars %>% 
  show_missings() %>% 
  mutate(mpg = ifelse(mpg < 20, NA, mpg)) %>% #inserts NAs everywhere mpg < 20
  show_missings() 
```


#### 19.7 Environment

The last component of a function is its environment. This is not something you need to understand deeply when you first start writing functions. However, it’s important to know a little bit about environments because they are crucial to how functions work. The environment of a function controls how R finds the value associated with a name. For example, take this function:
```{r}
f <- function(x) {
  x + y
}
```

In many programming languages, this would be an error, because y is not defined inside the function. In R, this is valid code because R uses rules called lexical scoping to find the value associated with a name. Since y is not defined inside the function, R will look in the environment where the function was defined:
```{r}
y <- 100
f(10)
#> [1] 110

y <- 1000
f(100)
#> [1] 1010
```

However, this isn't a great idea for a novice like me. Learn more about the environment and the differences that R allows in Advanced R: http://adv-r.had.co.nz/.


# ----*Chapter 20: Vectors*-----

Vectors underlie tibbles, in that they are the most basic R data objects and are the simplest way to store more than one value. There are two types of vectors:

1. Atomic vectors, of which there are six types: logical (TRUE | FALSE | NA), integer, double, character, complex, and raw. Integer and double vectors are collectively known as numeric vectors. 
2. Lists, which are sometimes called recursive vectors because lists can contain other lists.

The chief difference between atomic vectors and lists is that atomic vectors are homogeneous (every item in the vector is of the same data type), while lists can be heterogeneous (can contain strings & numbers. An atomic vector can not have a mix of different types because the type is a property of the complete vector, not the individual elements. If you need to mix multiple types in the same vector, you should use a list, which you’ll learn about shortly.

One other consideration is NULL, which is often used to represent the absence of a vector (as opposed to NA which is used to represent the absence of a value in a vector). NULL typically behaves like a vector of length 0.

Every vector has two key properties:

1. Its type, which you can determine with typeof().
```{r}
typeof(letters)
#> [1] "character"
typeof(1:10)
#> [1] "integer"
```

2. Its length, which you can determine with length().
```{r}
length(letters)
length(1:10)

x <- list("a", "b", 1:10)
length(x)
```

Why is letters of length 26? It's not b.c. the word is 26 items long. Instead, "letters" is a keyword that represents the letters of the English alphabet (or prob. influenced by location).

```{r}
letters
#length(apple) note that this won't work, so use nchar() instead
nchar("apple")
```

Vectors can also contain arbitrary additional metadata in the form of attributes. These attributes are used to create augmented vectors which build on additional behaviour. There are three important types of augmented vector:

* Factors are built on top of integer vectors.
* Dates and date-times are built on top of numeric vectors.
* Data frames and tibbles are built on top of lists.
This chapter will introduce you to these important vectors from simplest to most complicated. You’ll start with atomic vectors, then build up to lists, and finish off with augmented vectors.


#### 20.3 Important types of atomic vector

The four most important types of atomic vector are logical, integer, double, and character. Raw and complex are rarely used during a data analysis, so I won’t discuss them here.

##### 20.3.1 Logical vectors

Can only take 3 values: TRUE, FALSE, or NA. They are either created by comparisons or by hand:
```{r}
1:10 %% 3 == 0

c(TRUE, TRUE, FALSE, NA)
```

##### 20.3.2 Numeric

Integer and double vectors are known collectively as numeric vectors. In R, numbers are doubles by default. To make an integer, place an L after the number:
```{r}
typeof(1)
typeof(1L)
1.5L
```

The difference between integers and doubles doesn't matter in most cases, but there are 2 differences to remember:
1. Doubles are approximations. Doubles represent floating point numbers that can not always be precisely represented with a fixed amount of memory. This means that you should consider all doubles to be approximations. For example, what is square of the square root of two?
```{r}
x <- sqrt(2) ^ 2
x
x - 2 #to show that it's not ACTUALLY == 2, just a bit less b.c. of rounding error.
```

This behaviour is common when working with floating point numbers: most calculations include some approximation error. Instead of comparing floating point numbers using ==, you should use dplyr::near() which allows for some numerical tolerance.

2. Integers have one special value: NA, while doubles have four: NA, NaN, Inf and -Inf. All three special values NaN, Inf and -Inf can arise during division:
```{r}
test <- c(-1, 0, 1) / 0
test
```

Avoid using == to check for these other special values. Instead use the helper functions is.finite(), is.infinite(), and is.nan():
```{r}
is.infinite(test[1:3])
is.nan(test[1:3])
is.finite(test[1:3])
```

##### 20.3.3 Character vectors

Character vectors are the most complex type of atomic vector, because each element of a character vector is a string, and a string can contain an arbitrary amount of data.

You’ve already learned a lot about working with strings in strings. Here I wanted to mention one important feature of the underlying string implementation: R uses a global string pool. This means that each unique string is only stored in memory once, and every use of the string points to that representation. This reduces the amount of memory needed by duplicated strings. You can see this behaviour in practice with pryr::object_size():

```{r}
x <- "This is a reasonably long string."
pryr::object_size(x)

nchar(x)

y <- rep(x, 1000)
pryr::object_size(y)
#> 8.14 kB
```

##### 20.3.4 Missing values

Each type of atomic vector has it's own type of missing value. Normally you don’t need to know about these different types because you can always use NA and it will be converted to the correct type using the implicit coercion rules described next. However, there are some functions that are strict about their inputs, so it’s useful to have this knowledge sitting in your back pocket so you can be specific when needed.
```{r}
NA            # logical
#> [1] NA
NA_integer_   # integer
#> [1] NA
NA_real_      # double
#> [1] NA
NA_character_ # character
#> [1] NA
```

##### 20.3.5 Exercises

1. Describe the difference between is.finite(x) and !is.infinite(x).

```{r}
x <- c(1, 3, 5, Inf, NA)
is.finite(x)
!is.infinite(x)
```

It looks like the difference is in how they treat NAs.

2. Read the source code for dplyr::near() (Hint: to see the source code, drop the ()). How does it work?
```{r}
dplyr::near
```

It compares 2 numbers and sees whether they're w/in some set tolerance (which is a very small number)

3. A logical vector can take 3 possible values. How many possible values can an integer vector take? How many possible values can a double take? Use google to do some research.

<a lot>

4. Brainstorm at least four functions that allow you to convert a double to an integer. How do they differ? Be precise.
a. floor()
b. ceiling()
c. trunc()
d. round2() (according to sol'n page)

5. What functions from the readr package allow you to turn a string into logical, integer, and double vector?
See vignette at https://readr.tidyverse.org/articles/readr.html. parse_logical(), parse_integer(), parse_double().


#### 20.4 Using atomic vectors

Now that you understand the different types of atomic vector, it’s useful to review some of the important tools for working with them. These include:

1. Coercion: How to convert from one type to another, and when that happens automatically.
2. Test functions: How to tell if an object is a specific type of vector.
3. Scalars & recycling rules: What happens when you work with vectors of different lengths.
4. Naming: How to name the elements of a vector.
5. Subsetting: How to pull out elements of interest.

##### 20.4.1 Coercion

Conversion by coercion happens in 2 ways:
1. Explicitly, by calling a function like as.logical(), as.integer(), as.double(), or as.character(). Whenever you find yourself using explicit coercion, you should always check whether you can make the fix upstream, so that the vector never had the wrong type in the first place. For example, you may need to tweak your readr col_types specification.

2. Implicit coercion happens when you use a vector in a specific context that expects a certain type of vector. For example, when you use a logical vector with a numeric summary function, or when you use a double vector where an integer vector is expected. We'll focus on this, since explicit coercion is fairly straightforward.

In this case TRUE is converted to 1 and FALSE converted to 0. That means the sum of a logical vector is the number of trues, and the mean of a logical vector is the proportion of trues:
```{r}
x <- sample(100, 100, replace = TRUE) #pick a sample from 1-100, do it 100 times
y <- x  < 10
sum(y)  # how many are greater than 10?
#> the answer will differ each time
mean(y) # what proportion are greater than 10?

x
```

when you try and create a vector containing multiple types with c(): the most complex type always wins:
```{r}
typeof(c(TRUE, 1L))
#> [1] "integer"
typeof(c(1L, 1.5))
#> [1] "double"
typeof(c(1.5, "a"))
#> [1] "character"
```

##### 20.4.2 Test functions

Sometimes you want to do different things based on the type of vector. One option is to use typeof(). Another is to use a test function which returns a TRUE or FALSE. Base R provides many functions like is.vector() and is.atomic(), but they often return surprising results. Instead, it’s safer to use the is_* functions provided by purrr:
is_logical(), is_double(), is_numeric(), is_character(), is_atomic(), is_list(), and is_vector().

##### 20.4.3 Scalars and recycling rules

As well as implicitly coercing the types of vectors to be compatible, R will also implicitly coerce the length of vectors. This is called *vector recycling*, because the shorter vector is repeated, or recycled, to the same length as the longer vector.

This is generally most useful when you are mixing vectors and “scalars”. I put scalars in quotes because R doesn’t actually have scalars: instead, a single number is a vector of length 1. Because there are no scalars, most built-in functions are *vectorised*, meaning that they will operate on a vector of numbers. That’s why, for example, this code works:
```{r}
sample(10) + 100
#>  [1] 107 104 103 109 102 101 106 110 105 108
runif(10) > 0.5
#>  [1] FALSE  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
```

In R, basic mathematical operations work with vectors. That means that you should never need to perform explicit iteration when performing simple mathematical computations.
```{r}
x <- c(1, 3, 5, 7, 9)
y <- (1:10)

x + y
```

Here, R will expand the shortest vector to the same length as the longest, so called recycling. This is silent except when the length of the longer is not an integer multiple of the length of the shorter:
```{r}
1:10 + 1:3
```

the vectorised functions in tidyverse will throw errors when you recycle anything other than a scalar. If you do want to recycle, you’ll need to do it yourself with rep():
```{r}
#tibble(x = 1:4, y = 1:2) #won't work

tibble(x = 1:4, y = rep(1:2, 2)) #will work

tibble(x = 1:4, y = rep(1:2, each = 2)) #also will
```

##### 20.4.4 Naming vectors

All types of vectors can be named. You can name them during creation with c():
```{r}
c(x = 1, y = 2, z = 4)
```

or after the fact with purr::set_names():
```{r}
set_names(1:3, c("a", "b", "c"))
```

Named vectors are most useful for subsetting, described next.

##### 20.4.5 Subsetting vectors

So far we’ve used dplyr::filter() to filter the rows in a tibble. filter() only works with tibble, so we’ll need new tool for vectors: [. [ is the subsetting function, and is called like x[a]. There are four types of things that you can subset a vector with:

1. A numeric vector containing only integers. The integers must either be all positive, all negative, or zero.

Subsetting with positive integers keeps the elements at those positions:
```{r}
x <- c("one", "two", "three", "four", "five")
x[c(3, 2, 5)]
```

By repeating a position, you can actually make a longer output than input:
```{r}
x[c(3, 3, 3, 3, 3, 5, 5, 5, 5, 2, 2)]
```

Negative values drop the elements at the specified positions:
```{r}
x[c(-2, -5, -3)]
```

It’s an error to mix positive and negative values:
```{r}
#x[c(1, -1)]
```

And 0 doesn't work, b.c. this shit ain't no Python
```{r}
x[c(0)]
```

2. Subsetting with a logical vector keeps all values corresponding to a TRUE value. This is most often useful in conjunction with the comparison functions.
```{r}
x <- c(10, 3, NA, 5, 8, 1, NA)

# All non-missing values of x
x[!is.na(x)]
#> [1] 10  3  5  8  1

# All even (or missing!) values of x
x[x %% 2 == 0]
#> [1] 10 NA  8 NA
```

3. If you have a named vector, you can subset it with a character vector:
```{r}
x <- c(abc = 1, def = 2, xyz = 5)
x[c("xyz", "def")] #calls just 2 vectors
x[c("xyz", "xyz", "abc")] #can call multiple times
```

4. The simplest type of subsetting is nothing, x[], which returns the complete x. This is not useful for subsetting vectors, but it is useful when subsetting matrices (and other high dimensional structures) because it lets you select all the rows or all the columns, by leaving that index blank. For example, if x is 2d, x[1, ] selects the first row and all the columns, and x[, -1] selects all rows and all columns except the first.

To learn more about the applications of subsetting, reading the “Subsetting” chapter of Advanced R: http://adv-r.had.co.nz/Subsetting.html#applications.

There is an important variation of [ called [[. [[ only ever extracts a single element, and always drops names. It’s a good idea to use it whenever you want to make it clear that you’re extracting a single item, as in a for loop. The distinction between [ and [[ is most important for lists, as we’ll see shortly.

##### 20.4.6 Exercises

1. What does mean(is.na(x)) tell you about a vector x? What about sum(!is.finite(x))?
This is from sol'n page:

The expression mean(is.na(x)) calculates the proportion of missing (NA) and not-a-number NaN values in a vector.
The expression sum(!is.finite(x)) calculates the number of elements in the vector that are equal to missing (NA), not-a-number (NaN), or infinity (Inf).
```{r}
x <- c(-Inf, -1, 0, 1, Inf, NA, NaN)
mean(is.na(x))
sum(!is.finite(x))
```

2. Carefully read the documentation of is.vector(). What does it actually test for? Why does is.atomic() not agree with the definition of atomic vectors above?
```{r}
is.vector()
is.atomic()
```

3. Compare and contrast setNames() with purrr::set_names()
```{r}
setNames()
purrr::set_names()
```
The biggest difference between set_names() and setNames() is that set_names() allows for using a function or formula to transform the existing names. There are several other diffs in the sol'n page

4. Create functions that take a vector as input and returns:

1. The last value. Should you use [ or [[?
```{r}
last_val <- function(x) {
  x[[length(x)]] #choose the value that's at the last pos'n
}

vec1
last_val(vec1)
```

Sol'n has more thorough answer that checks for zero length:
```{r}
last_value <- function(x) {
  # check for case with no length
  if (length(x)) {
    x[[length(x)]] 
  } else {
    x
  }
}
last_value(numeric())
#> numeric(0)
last_value(1)
#> [1] 1
last_value(1:10)
#> [1] 10
```

2. The elements at even numbered positions. NOT even numbers!

```{r}
find_even <- function(x) {
  if (length(x)) {
    x[seq_along(x) %% 2 == 0]
  } else {
    x
  }
}

find_even(numeric())
#> numeric(0)
find_even(200)
#> [1] 1
find_even(1:10)
#> [1] 10
```

3. Every element except the last value.

```{r}
remove_last <- function(x) {
  n <- length(x)
  if (n) {
    x[-n]
  } else {
    x
  }
}

remove_last(1:10)
```

4. Only even numbers (and no missing values).
```{r}
find_even <- function(x) {
  if (length(x)) {
    x[x %% 2 == 0]
  } else {
    x
  }
}

find_even(1:20)
```


##### 20.5 Recursive vectors (lists)

Lists are a step up in complexity from atomic vectors, because lists can contain other lists. This makes them suitable for representing hierarchical or tree-like structures. You create a list with list():
```{r}
x <- list(1, 2, 3)
x
```

A very useful tool for working with lists is str() because it focusses on the *str*ucture, not the contents.
```{r}
str(x)
```

You can also make lists with names:
```{r}
x_named <- list(a = 1, b = 2, c = 3)
str(x_named)
```

The big difference between atomic vectors and lists is that a list() can contain a mix of types:
```{r}
y <- list("a", 1L, 1.5, TRUE)
str(y)
```

Where it gets hairy is that lists can contain other lists:
```{r}
z <- list(list(1, 2), list(3, 4))
str(z)
```

##### 20.5.1 Visualising lists

Lists can get complicated, so it helps to have a visual representation. There is a nice image of these three lists in R4DS.
```{r}
x1 <- list(c(1, 2), c(3, 4))
x2 <- list(list(1, 2), list(3, 4))
x3 <- list(1, list(2, list(3)))
```

```{r}
str(x1)
str(x2)
str(x3)
```

Using str() can be helpful for seeing the structure, but it's more helpful at first to try to draw them out.

##### 20.5.2 Subsetting
There are three ways to subset a list, which I’ll illustrate with a list named a:
```{r}
a <- list(a = 1:3, b = "a string", c = pi, d = list(-1, -5))
str(a)
```

[ extracts a sub-list. The result will always be a list.
```{r}
str(a[1:2]) #this pulls out just the first 2 elements of list a
a[1:2]
str(a[4]) # this pulls out just the 4th element, which is a list of 2 items
a[4]
```

Like with vectors, you can subset with a logical, integer, or character vector.

[[ extracts a single component from a list. It removes a level of hierarchy from the list.
```{r}
str(a[[1]])
a[[1]]

str(a[[4]]) #what's different from above is that this does not produce a list
a[[4]]
```

##### extracting named elements
$ is a shorthand for extracting named elements of a list. It works similarly to [[ except that you don’t need to use quotes.
```{r}
a$a #pull element a from list a

a[["a"]]
```

The distinction between [ and [[ is really important for lists, because [[ drills down into the list while [ returns a new, smaller list. See the book for a visual.
```{r}
#these 3 all generate lists
a
a[1:2]
a[4]
```

```{r}
#these 3 all pull elements of lists
a[[4]]
a[[4]][[1]]
```

Subsetting a tibble works the same way as a list; a data frame can be thought of as a list of columns. The key difference between a list and a tibble is that all the elements (columns) of a tibble must have the same length (number of rows). Lists can have vectors with different lengths as elements.


#### 20.6 Attributes 

Any vector can contain arbitrary additional metadata through its attributes. You can think of attributes as named list of vectors that can be attached to any object. You can get and set individual attribute values with attr() or see them all at once with attributes().
```{r}
x <- 1:10
attr(x, "greeting") #it doesn't have any attributes yet
#> NULL
attr(x, "greeting") <- "Hi!" #assign "Hi!" to the attribute "greeting"
attr(x, "farewell") <- "Bye!"
attributes(x) #see all attributes of x
```

There are three very important attributes that are used to implement fundamental parts of R:

1. Names are used to name the elements of a vector.
2. Dimensions (dims, for short) make a vector behave like a matrix or array.
3. Class is used to implement the S3 object oriented system.

You’ve seen names above, and we won’t cover dimensions because we don’t use matrices in this book. It remains to describe the class, which controls how generic functions work. Generic functions are key to object oriented programming in R, because they make functions behave differently for different classes of input. A detailed discussion of object oriented programming is beyond the scope of this book, but you can read more about it in Advanced R at http://adv-r.had.co.nz/OO-essentials.html#s3.

Here’s what a typical generic function looks like:

```{r}
as.Date
```

The call to “UseMethod("as.Date")” means that this is a generic function, and it will call a specific method, a function, based on the class of the first argument. (All methods are functions; not all functions are methods). You can list all the methods for a generic with methods():
```{r}
methods("as.Date")
```

For example, if x is a character vector, as.Date() will call as.Date.character(); if it’s a factor, it’ll call as.Date.factor().

You can see the specific implementation of a method with getS3method():
```{r}
getS3method("as.Date", "default")
```

#### 20.7 Augmented vectors

Atomic vectors and lists are the building blocks for other important vector types like factors and dates. I call these augmented vectors, because they are vectors with additional attributes, including class. Because augmented vectors have a class, they behave differently to the atomic vector on which they are built. In this book, we make use of four important augmented vectors:

1. Factors
2. Dates
3. Date-times
4. Tibbles

##### 20.7.1 Factors

Factors are designed to represent categorical data that can take a fixed set of possible values. Factors are built on top of integers, and have a levels attribute:
```{r}
x <- factor(c("ab", "cd", "ab"), levels = c("ab", "cd", "ef"))
typeof(x)
#> [1] "integer"
attributes(x)
```

##### 20.7.2 Dates and date-times

Dates in R are numeric vectors that represent the number of days since 1 January 1970.
```{r}
x <- as.Date("1971-01-01")
unclass(x)
#> [1] 365

typeof(x)
#> [1] "double"
attributes(x)
#> $class
#> [1] "Date"
```

Date-times are numeric vectors with class POSIXct that represent the number of seconds since 1 January 1970. (In case you were wondering, “POSIXct” stands for “Portable Operating System Interface”, calendar time.)
```{r}
x <- lubridate::ymd_hm("1970-01-01 01:00")
unclass(x)
#> [1] 3600
#> attr(,"tzone")
#> [1] "UTC"

typeof(x)
#> [1] "double"
attributes(x)
```

##### 20.7.3 Tibbles

Tibbles are augmented lists: they have class “tbl_df” + “tbl” + “data.frame”, and names (column) and row.names attributes:
```{r}
tb <- tibble::tibble(x = 1:5, y = 5:1)
typeof(tb)
#> [1] "list"
attributes(tb)
```

The difference between a tibble and a list is that all the elements of a data frame must be vectors with the same length. All functions that work with tibbles enforce this constraint.

Traditional data.frames have a very similar structure:
```{r}
df <- data.frame(x = 1:5, y = 5:1)
typeof(df)
#> [1] "list"
attributes(df)
```

The main difference is the class. The class of tibble includes “data.frame” which means tibbles inherit the regular data frame behaviour by default.


# ----*Chapter 21: Iteration*-----

Like functions, iteration allows you to make your code reproducible, easy to update, less prone to minor errors, and also faster. In this chapter you’ll learn about two important iteration paradigms: imperative programming and functional programming. On the imperative side you have tools like for loops and while loops, which are a great place to start because they make iteration very explicit, so it’s obvious what’s happening. However, for loops are quite verbose, and require quite a bit of bookkeeping code that is duplicated for every for loop. Functional programming (FP) offers tools to extract out this duplicated code, so each common for loop pattern gets its own function. Once you master the vocabulary of FP, you can solve many common iteration problems with less code, more ease, and fewer errors.

#### 21.2 For loops

Starting with a simple tibble:
```{r}
df <- tibble(
  a = rnorm(10),
  b = rnorm(10),
  c = rnorm(10),
  d = rnorm(10)
)
df
```

To compute the median, you could copy and paste the appropriate code:
```{r}
median(df$a)
#> [1] -0.2457625
median(df$b)
#> [1] -0.2873072
median(df$c)
#> [1] -0.05669771
median(df$d)
#> [1] 0.1442633
```

A better alternative is a for loop:
```{r}
output <- vector("double", ncol(df))  # 1. output
for (i in seq_along(df)) {            # 2. sequence
  output[[i]] <- median(df[[i]])      # 3. body
}
output
```

Every for loop has 3 components:
1. the output: output <- vector("double", length(x)). Before you start the loop, you must always allocate sufficient space for the output. This is very important for efficiency: if you grow the for loop at each iteration using c() (for example), your for loop will be very slow.

A general way of creating an empty vector of given length is the *vector() function*. It has two arguments: the type of the vector (“logical”, “integer”, “double”, “character”, etc) and the length of the vector.

2. The sequence: i in seq_along(df). This determines what to loop over: each run of the for loop will assign i to a different value from seq_along(df). It’s useful to think of i as a pronoun, like “it”.

You might not have seen seq_along() before. It’s a safe version of the familiar 1:length(l), with an important difference: if you have a zero-length vector, seq_along() does the right thing. If you use 1:length(x) instead of seq_along(x), you’re likely to get a confusing error message.
```{r}
y <- vector("double", 0)
seq_along(y)
#> integer(0)
1:length(y)
#> [1] 1 0
```

3. The body: output[[i]] <- median(df[[i]]). This is the code that does the work. It’s run repeatedly, each time with a different value for i. The first iteration will run output[[1]] <- median(df[[1]]), the second will run output[[2]] <- median(df[[2]]), and so on.


##### 21.2.1 Exercises

1. Write for loops to:
1.1. Compute the mean of every column in mtcars.
```{r ex 1.1}
output <- vector("double", ncol(mtcars))  # 1. output
for (i in seq_along(mtcars)) {            # 2. sequence
  output[[i]] <- mean(mtcars[[i]])      # 3. body
}
output
```

1.2 Determine the type of each column in nycflights13::flights.
```{r Ex 1.2}
output <- vector("character", ncol(nycflights13::flights))
for (i in seq_along(nycflights13::flights)) {
  output[[i]] <- typeof(nycflights13::flights[[i]])
}
output
```

1.3 Compute the number of unique values in each column of iris.
```{r ex 1.3}
unique_vals <- vector("integer", ncol(iris))
names(unique_vals) <- names(iris) #this line prints the names of the columns to match the output from the for loop
for (i in seq_along(iris)) {
  unique_vals[[i]] <- n_distinct(iris[[i]])
}
unique_vals
```

1.4 To generate 10 random normals for each of mean = -10, 0, 10, and 100.
```{r ex 1.4}
n <- 10 #set the number of draws
mu <- c(-10, 0, 10, 100)
normals <- vector("list", length(mu)) #use list here b.c. 
for (i in seq_along(normals)) {
  normals[[i]] <- rnorm(n, mean = mu[i])
}
normals
```

2.1. Eliminate the for loop in each of the following examples by taking advantage of an existing function that works with vectors:
```{r}
out <- ""
for (x in letters) {
  out <- stringr::str_c(out, x)
}
out
```

```{r}
str_flatten(letters)
```

skipped the rest of #2

3. Combine your function writing and for loop skills:

3.1. Write a for loop that prints() the lyrics to the children’s song “Alice the camel”.
This is what sol'n guy wrote:
```{r ex 3.1}
humps <- c("five", "four", "three", "two", "one", "no")
for (i in humps) {
  cat(str_c("Alice the camel has ", rep(i, 3), " humps.", #rep() repeats i 3 times
    collapse = "\n"
  ), "\n")
  if (i == "no") {
    cat("Now Alice is a horse.\n")
  } else {
    cat("So go, Alice, go.\n")
  }
  cat("\n")
}
```

3.2 Convert the nursery rhyme “ten in the bed” to a function. Generalise it to any number of people in any sleeping structure.
Sol'n guy's sol'n:
```{r ex. 3.2}
#first, establish the array to loop over
numbers <- c(
  "ten", "nine", "eight", "seven", "six", "five",
  "four", "three", "two", "one"
)
for (i in numbers) {
  cat(str_c("There were ", i, " in the bed\n"))
  cat("and the little one said\n")
  if (i == "one") {
    cat("I'm lonely...")
  } else {
    cat("Roll over, roll over\n")
    cat("So they all rolled over and one fell out.\n")
  }
  cat("\n")
}
```


3.3 Convert the song “99 bottles of beer on the wall” to a function. Generalise to any number of any vessel containing any liquid on any surface.
My notes & attempt:
1. function has to have 4 inputs: # of vessels, type of vessel, type of liquid, and surface type.
The input piece should be song_round <- function(n, vessel, liquid, surface) 
This is actually quite a complex ask, and I don't want to spend that much time on it.

4. It’s common to see for loops that don’t preallocate the output and instead increase the length of a vector at each step:
I just want to see what sol'n guy did here to learn about tracking calculation times.

Guy: "In order to compare these two approaches, I’ll define two functions: add_to_vector will append to a vector, like the example in the question, and add_to_vector_2 which pre-allocates a vector."
```{r}
add_to_vector <- function(n) {
  output <- vector("integer", 0)
  for (i in seq_len(n)) {
    output <- c(output, i)
  }
  output
}

add_to_vector_2 <- function(n) {
  output <- vector("integer", n)
  for (i in seq_len(n)) {
    output[[i]] <- i
  }
  output
}
```

Now, use the package microbenchmark to run these functions several times and compare the time it takes. The package microbenchmark contains utilities for benchmarking R expressions. In particular, the microbenchmark() function will run an R expression a number of times and time it.
```{r}
library("microbenchmark")
timings <- microbenchmark(add_to_vector(10000), add_to_vector_2(10000), times = 10)
timings
```
In this example, appending to a vector takes 325 times longer than pre-allocating the vector. (wow!)


#### 21.3 For loop variations

There are four variations on the basic theme of the for loop:

1. Modifying an existing object, instead of creating a new object.
2. Looping over names or values, instead of indices.
3. Handling outputs of unknown length.
4. Handling sequences of unknown length.

##### 21.3.1 Modifying an existing object

Sometimes you want to use a for loop to modify an existing object. For example, remember our challenge from functions. We wanted to rescale every column in a data frame:
```{r}
df <- tibble(
  a = rnorm(10),
  b = rnorm(10),
  c = rnorm(10),
  d = rnorm(10)
)
rescale01 <- function(x) {
  rng <- range(x, na.rm = TRUE)
  (x - rng[1]) / (rng[2] - rng[1])
}

df$a <- rescale01(df$a)
df$b <- rescale01(df$b)
df$c <- rescale01(df$c)
df$d <- rescale01(df$d)
```

To solve this with a for loop we again think about the three components:

1. Output: we already have the output — it’s the same as the input!
2. Sequence: we can think about a data frame as a list of columns, so we can iterate over each column with seq_along(df).
3. Body: apply rescale01().

This gives us:
```{r}
for (i in seq_along(df)) {
  df[[i]] <- rescale01(df[[i]])
}
```

Typically you’ll be modifying a list or data frame with this sort of loop, so remember to use [[, not [. You might have spotted that I used [[ in all my for loops: I think it’s better to use [[ even for atomic vectors because it makes it clear that I want to work with a single element.

##### 21.3.2 Looping patterns

There are three basic ways to loop over a vector. So far I’ve shown you the most general: *looping over the numeric indices* with for (i in seq_along(xs)), and *extracting the value* with x[[i]]. There are two other forms:

1. Loop over the elements: for (x in xs). This is most useful if you only care about side-effects, like plotting or saving a file, because it’s difficult to save the output efficiently.

2. Loop over the names: for (nm in names(xs)). This gives you name, which you can use to access the value with x[[nm]]. This is useful if you want to use the name in a plot title or a file name. If you’re creating named output, make sure to name the results vector like so:
```{r}
results <- vector("list", length(x))
names(results) <- names(x)
```

BUT, you can do the same thing by iterating over the numeric indices:
```{r}
for (i in seq_along(x)) {
  name <- names(x)[[i]]
  value <- x[[i]]
}
```

##### 21.3.3 Unknown output length

Sometimes you might not know how long the output will be. For example, imagine you want to simulate some random vectors of random lengths. You might be tempted to solve this problem by progressively growing the vector:
```{r}
means <- c(0, 1, 2)

output <- double()
for (i in seq_along(means)) {
  n <- sample(100, 1)
  output <- c(output, rnorm(n, means[[i]]))
}
str(output)
#>  num [1:138] 0.912 0.205 2.584 -0.789 0.588 ...
```

This is really slow, since after every iteration R saves the previous iteration. A better solution to save the results in a list, and then combine into a single vector after the loop is done:
```{r}
means <- c(0, 1, 2)
out <- vector("list", length(means))
for (i in seq_along(means)) {
  n <- sample(100, 1)
  out[[i]] <- rnorm(n, means[[i]])
}
str(out)

str(unlist(out))
```
This used unlist() to flatten a list of vectors into a single vector. A stricter option is to use purrr::flatten_dbl() — it will throw an error if the input isn’t a list of doubles.

This pattern occurs in other places too:

1. You might be generating a long string. Instead of paste()ing together each iteration with the previous, save the output in a character vector and then combine that vector into a single string with paste(output, collapse = "").

2. You might be generating a big data frame. Instead of sequentially rbind()ing in each iteration, save the output in a list, then use dplyr::bind_rows(output) to combine the output into a single data frame.

Watch out for this pattern. Whenever you see it, switch to a more complex result object, and then combine in one step at the end.

##### 21.3.4 Unknown sequence length (while loops)

A while loop is simpler than for loop because it only has two components, a condition and a body:
```{r while loops}
while (condition) {
  # body
}
```

A while loop is also more general than a for loop, because you can rewrite any for loop as a while loop, but you can’t rewrite every while loop as a for loop:

```{r while vs for}
for (i in seq_along(x)) {
  # body
}

# Equivalent to
i <- 1
while (i <= length(x)) {
  # body
  i <- i + 1 
}
```

However, they're only mostly used for simulations, so Hadley doesn't go into the details much.

##### 21.3.5 Exercises

1. Imagine you have a directory full of CSV files that you want to read in. You have their paths in a vector, files <- dir("data/", pattern = "\\.csv$", full.names = TRUE), and now want to read each one with read_csv(). Write the for loop that will load them into a single data frame.

This is actually a pretty useful example. I'll just give in to the temptation of annotating sol'n guy's answer:
```{r}
files <- dir("data/", pattern = "\\.csv$", full.names = TRUE)
files

#'Since, the number of files is known, pre-allocate a list with a length equal to the number of files.
df_list <- vector("list", length(files))

# Then, read each file into a data frame, and assign it to an element in that list. The result is a list of data frames.
for (i in seq_along(files)) {
  df_list[[i]] <- read_csv(files[[i]])
}

# Finally, use use bind_rows() to combine the list of data frames into a single data frame.

df <- bind_rows(df_list)
```


#### 21.4 For loops vs. functionals

For loops are not as important in R as they are in other languages because R is a functional programming language. This means that it’s possible to wrap up for loops in a function, and call that function instead of using the for loop directly.

To see why this is important, consider (again) this simple data frame:
```{r}
df <- tibble(
  a = rnorm(10),
  b = rnorm(10),
  c = rnorm(10),
  d = rnorm(10)
)
```

Computing the mean with a for loop (again):
```{r}
output <- vector("double", ncol(df))
for (i in seq_along(df)) {
  output[[i]] <- mean(df[[i]])
}
output
```

In this scenario, You realise that you’re going to want to compute the means of every column pretty frequently, so you extract it out into a function:
```{r}
col_mean <- function(df) {
  output <- vector("double", ncol(df))
for (i in seq_along(df)) {
  output[[i]] <- mean(df[[i]])
}
output
}
```

try it out to check:
```{r}
col_mean(df)
```

But then you think it’d also be helpful to be able to compute the median, and the standard deviation, so you copy and paste your col_mean() function and replace the mean() with median() and sd(). But now, you're violating the copy/paste rule, so think about generalizing this with a function. You can add an argument that supplies the function to apply to each column:
```{r}
col_summary <- function(df, fun) {
  out <- vector("double", length(df))
  for (i in seq_along(df)) {
    out[i] <- fun(df[[i]])
  }
  out
}
col_summary(df, median)
col_summary(df, mean)
#col_summary(df, sd) #this is throwing an error for some reason
```

The idea of passing a function to another function is an extremely powerful idea, and it’s one of the behaviours that makes R a functional programming language. This takes time to digest for some people. 

##### the purrrpose of purrr

*For other resources, see https://www.rebeccabarter.com/blog/2019-08-19_purrr/*

The purrr package allows provides functions that obviate the need for many for loops by passing functions to other functions. The apply family of functions in base R (apply(), lapply(), tapply(), etc) solve a similar problem, but purrr is more consistent and thus is easier to learn.

The goal of using purrr functions instead of for loops is to allow you to break common list manipulation challenges into independent pieces:

1. How can you solve the problem for a single element of the list? Once you’ve solved that problem, purrr takes care of generalising your solution to every element in the list.

2. If you’re solving a complex problem, how can you break it down into bite-sized pieces that allow you to advance one small step towards a solution? With purrr, you get lots of small pieces that you can compose together with the pipe.

This structure makes it easier to solve new problems. It also makes it easier to understand your solutions to old problems when you re-read your old code.

##### 21.4.1 Exercises
1. Read the documentation for apply(). In the 2d case, what two for loops does it generalise?

```{r setting a named array}
x <- cbind(x1 = 3, x2 = c(4:1, 2:5))
dimnames(x)[[1]] <- letters[1:8]
x
apply(x, 2, mean, trim = .2) #the 2 means apply over columns, 1 = do row-wise
```

#### 21.5 The *map* functions

Map functions are part of the purrr package. The map functions transform their input by applying a function to each element of a *list or atomic vector* and returning an object of the same length as the input. 

The usage (see documentation here: https://purrr.tidyverse.org/reference/map.html) in my longhand notation is map_*(atomic vector or list, function or formula to apply, other optional arguments needed for the function). The shorthand notation is map(.x, .f, ...)

The pattern of looping over a vector, doing something to each element and saving the results is so common that the purrr package provides a family of functions to do it for you. There is one function for each type of output:

* map() always returns a list.
* map_lgl() makes a logical vector.
* map_int() makes an integer vector.
* map_dbl() makes a double vector.
* map_chr() makes a character vector.

Each function takes a vector as input, applies a function to each piece, and then returns a new vector that’s the same length (and has the same names) as the input. The type of the vector is determined by the suffix to the map function.

Once you master these functions, you’ll find it takes much less time to solve iteration problems. But you should never feel bad about using a for loop instead of a map function. The map functions are a step up a tower of abstraction, and it can take a long time to get your head around how they work.

There's not a difference in speed between for loops and map functions, but the big benefit is clarity of code. 

Here's an example of using map_dbl() to calculate means, medians, & sds:
```{r}
df <- tibble(
  a = rnorm(10),
  b = rnorm(10),
  c = rnorm(10),
  d = rnorm(10)
)

map_dbl(df, mean) #specifying array
df |> map_dbl(median) #using base pipe
#df %>% map_dbl(sd) #this isn't working
```

##### 21.5.1 Shortcuts with map() & pipes

There are a few shortcuts that you can use with .f (the function, formula, or vector to be applied) in order to save a little typing. Imagine you want to fit a linear model to each group in a dataset. The following toy example splits up the mtcars dataset into three pieces (one for each value of cylinder) and fits the same linear model to each piece:

```{r the long way}
models <- mtcars %>% 
  split(.$cyl) %>% #splits df by cyl
  map(function(df) lm(mpg ~ wt, data = df)) #runs a linear model for each cyl type
models #shows the output for each cyl type (4, 6, etc.)
```

The syntax for creating an anonymous function in R is quite verbose so purrr provides a convenient shortcut: a one-sided formula.

```{r shortcut w/ ~ as 1-sided formula}
models <- mtcars %>% 
  split(.$cyl) %>% 
  map(~lm(mpg ~ wt, data = .)) #here, the . just refers to the current list element
```

```{r the base pipe version}
models <- mtcars |> 
  split(mtcars$cyl) |> #can't use .$cyl here
  map(~lm(mpg ~ wt, data = .x)) #have to use .x
```

Here I’ve used .x as a pronoun (just . in the %>% version): it refers to the current list element (in the same way that i referred to the current index in the for loop). .x in a one-sided formula corresponds to an argument in an anonymous function.

When you’re looking at many models, you might want to extract a summary statistic like the 
R^2. To do that we need to first run summary() and then extract the component called r.squared. We could do that using the shorthand for anonymous functions:
```{r}
models %>% 
  map(summary) %>% 
  map_dbl(~.$r.squared)
```

But extracting named components is a common operation, so purrr provides an even shorter shortcut: you can use a string.
```{r}
models %>%
  map(summary) %>%
  map_dbl("r.squared")
```

You can also use an integer to select elements by position:
```{r}
x <- list(list(1, 2, 3), list(4, 5, 6), list(7, 8, 9))
x %>% map_dbl(2)
```

##### 21.5.2 Base R apply() and purrr

* lapply() is basically identical to map(), except that map() is consistent with all the other functions in purrr, and you can use the shortcuts for .f.

* Base sapply() is a wrapper around lapply() that automatically simplifies the output. This is useful for interactive work but is problematic in a function because you never know what sort of output you’ll get.

* vapply() is a safe alternative to sapply() because you supply an additional argument that defines the type. The only problem with vapply() is that it’s a lot of typing: vapply(df, is.numeric, logical(1)) is equivalent to map_lgl(df, is.numeric). One advantage of vapply() over purrr’s map functions is that it can also produce matrices — the map functions only ever produce vectors.

R4DS focuses on purrr functions here because they have more consistent names and arguments, helpful shortcuts, and in the future will provide easy parallelism and progress bars.

##### 21.5.3 Exercises

1. Write code that uses one of the map functions to:
a. Compute the mean of every column in mtcars.
```{r map mean}
mtcars %>%
  map_dbl(mean)
#or
map_dbl(mtcars, mean)
```

b. Determine the type of each column in nycflights13::flights.
```{r map typeof}
map_chr(nycflights13::flights, typeof)
```

c. Compute the number of unique values in each column of iris.
```{r map n_distinct}
map(iris, n_distinct)
```

d. Generate 10 random normals from distributions with means of -10, 0, 10, and 100.
```{r map anonymous funct}
nums <- c(-10, 0, 10, 100)
map(nums, ~rnorm(n = 10, mean = .)) #I don't fully get this, but I think the ~ and . combo applies the rnorm function to each item in the nums array
```

2. How can you create a single vector that for each column in a data frame indicates whether or not it’s a factor? Not a clear question, but sol'n answer is helpful:

Checking all columns in a data frame is a job for a map_*() function. Since the result of is.factor() is logical, we will use map_lgl() to apply is.factor() to the columns of the data frame.
```{r map logical}
map_lgl(iris, is.factor)
```

3. What happens when you use the map functions on vectors that aren’t lists? What does map(1:5, runif) do? Why? Again, not a clear question, but the sol'n page is useful.

```{r interesting shorthand uses of map}
# applies simple operation (not the element, or !.)
map(c(TRUE, FALSE, TRUE), ~ !.) 
# applies str_to_upper function to simple list of 2 strings
map(c("Hello", "World"), str_to_upper)
# generates a series of numbers from 1-3 and uses them as the number of obs for the rnorm function (where the default arguments are mean = 0, sd = 1)
map(1:3, ~ rnorm(.))
#applies rnorm to array of 3 numbers that = mean for rnorm, then specifies 1 = the n for the rnorm function 
map(c(-0.5, 0, 1), ~ rnorm(1, mean = .))
```

It is important to be aware that while the input of map() can be any vector, the output is always a list.

Addressing the question, then, map(1:5, runif) generates a series of numbers from 1-5 and generates that number's amount of uniform distributions using defaults
```{r}
map(1:5, runif)
```

4. What does map(-2:2, rnorm, n = 5) do? Why? What does map_dbl(-2:2, rnorm, n = 5) do? Why?
```{r}
#generates a list of 5 random numbers 
map(-2:2, rnorm, n = 5)
#below generates an error 
#map_dbl(-2:2, rnorm, n = 5)
```
The first expression takes samples of size five from five normal distributions, with means of (-2, -1, 0, 1, and 2), but the same standard deviation (1). It returns a list with each element a numeric vectors of length 5.

For the second expression, the map_dbl() function requires the function it applies to each element to return a numeric vector of length one. If the function returns either a non-numeric vector or a numeric vector with a length greater than one, map_dbl() will raise an error.

5. Rewrite map(x, function(df) lm(mpg ~ wt, data = df)) to eliminate the anonymous function.
I"m not really sure what part is the anonymous function here.
Another issue is that the reference code doesn't run, so sol'n uses a different example.
```{r}
#version w/ anonymous function
x <- split(mtcars, mtcars$cyl)
map(x, function(df) lm(mpg ~ wt, data = df))

#version w/o anonymous, but with ~ shortcut
map(x, ~ lm(mpg ~ wt, data = .))
```

#### 21.6 Dealing with failure with map()

In this section you’ll learn how to deal with this situation with a new function: safely(). safely() is an adverb: it takes a function (a verb) and returns a modified version. In this case, the modified function will never throw an error. Instead, it always returns a list with two elements:

1. result is the original result. If there was an error, this will be NULL.
2. error is an error object. If the operation was successful, this will be NULL.

Let’s illustrate this with a simple example: log():
```{r}
safe_log <- safely(log)
str(safe_log(10)) #can do successfully

str(safe_log("a")) #produces an error
```

When the function succeeds, the result element contains the result and the error element is NULL. When the function fails, the result element is NULL and the error element contains an error object.

safely() is designed to work with map:

```{r using safely()}
x <- list(1, 10, "a")
y <- x %>% map(safely(log))
str(y) #this asks for the structure of an object
```

What does the error look like when we try to do it anyway?
```{r}
#x <- list(1, 10, "a")
#map(x, log)

# produces: Error in .Primitive("log")(x, base) : 
#non-numeric argument to mathematical function
```

Purrr provides two other useful adverbs:

Like safely(), possibly() always succeeds. It’s simpler than safely(), because you give it a default value to return when there is an error.

```{r}
x <- list(1, 10, "a")
x %>% map_dbl(possibly(log, NA_real_))
```


#### 21.7 Mapping over multiple arguments

So far we’ve mapped along a single input. But often you have multiple related inputs that you need iterate along in parallel. That’s the job of the map2() and pmap() functions. For example, imagine you want to simulate some random normals with different means. You know how to do that with map():
```{r}
mu <- c(1, 10, 100, 1000)
means %>%
  map(rnorm, n = 5) %>%
  str()
```

What if you also want to vary the standard deviation? One way to do that would be to iterate over the indices and index into vectors of means and sds. But that obfuscates the intent of the code. Instead we could use map2() which iterates over *two vectors* in parallel:
```{r using map2}
sigma <- list(1, 5, 10, 100)
map2(mu, sigma, rnorm, n = 5) %>% 
  str()
```

Note that the arguments that vary for each call come *before* the function; arguments that are the same for every call come *after*.

You could also imagine map3(), map4(), map5(), map6() etc, but that would get tedious quickly. Instead, purrr provides pmap() which takes a list of arguments. You might use that if you wanted to vary the mean, standard deviation, and number of samples:

```{r multiple arguments with pmap}
n <- list(1, 3, 5, 7)
args1 <- list(n, mu, sigma)
args1 %>%
  pmap(rnorm) %>% 
  str()
```

If you don’t name the list’s elements, pmap() will use positional matching when calling the function. That’s a little fragile, and makes the code harder to read, so it’s better to name the arguments:
```{r}
args2 <- list(mean = mu, sd = sigma, n = n)
args2 %>% 
  pmap(rnorm) %>% 
  str()
```

Since the arguments are all the same length, it makes sense to store them in a data frame:
```{r pmap arguments in tibble}
params <- tribble(
  ~mean, ~sd, ~n,
    5,     1,  1,
   10,     5,  3,
   -3,    10,  5
)
params %>% 
  pmap(rnorm)
```

##### 21.7.1 Invoking different functions

There’s one more step up in complexity - as well as varying the arguments to the function you might also vary the function itself:
```{r mapping multiple functions}
f <- c("runif", "rnorm", "rpois") #an array of functions to run through map
param <- list(         # parameterizing the functions w/ arguments
  list(min = -1, max = 1), 
  list(sd = 5), 
  list(lambda = 10)
)
```

Then invoke mapping:
```{r}
invoke_map(f, param, n = 5) %>% str()
```

Above, f gives the list of functions to run or character vector of function names. param is the list of lists that vary for each function. 

Invoking tibbles can make this easier & cleaner:
```{r tibble of multiple functions}
sim <- tribble(
  ~f,      ~params,
  "runif", list(min = -1, max = 1),
  "rnorm", list(sd = 5),
  "rpois", list(lambda = 10)
)
sim %>% 
  mutate(sim = invoke_map(f, params, n = 10))
```


#### 21.8 Walk

Walk is an alternative to map that you use when you want to call a function for its side effects, rather than for its return value. You typically do this because you want to render output to the screen or save files to disk - the important thing is the action, not the return value. Here’s a very simple example:
```{r}
x <- list(1, "a", 3)

x %>% 
  walk(print)
```

walk() is generally not that useful compared to walk2() or pwalk(). For example, if you had a list of plots and a vector of file names, you could use pwalk() to save each file to the corresponding location on disk:
```{r}
library(ggplot2)
plots <- mtcars %>% 
  split(.$cyl) %>% 
  map(~ggplot(., aes(mpg, wt)) + geom_point())
paths <- stringr::str_c(names(plots), ".pdf")

pwalk(list(paths, plots), ggsave, path = tempdir())
```


# ----*For the remainder of the book, see the following*----

* Then for model basics, model building, and many models, see R4DS_code_Model(22-25).Rmd

* Finally is R Markdown, Graphics for communication, R Markdown formats, and R Markdown workflow, which are in the file R4DS_code_Communicate(26-30).Rmd





